{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# IMPORT KERAS LIBRARY\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, GlobalAveragePooling2D, MaxPooling2D, Flatten, BatchNormalization, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "# vit architecture\n",
    "%pip install vit-keras\n",
    "# https://github.com/faustomorales/vit-keras\n",
    "%pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path='.'\n",
    "path=r'D:\\Datasets\\CheXpert-v1.0-small'\n",
    "train_folder=f'{path}train'\n",
    "test_folder=f'{path}test'\n",
    "train_lbl=f'{path}train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chestxrays_root = Path(path)\n",
    "data_path = chestxrays_root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and the targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_df = pd.read_csv(data_path/'CheXpert-v1.0-small/train.csv')\n",
    "full_valid_df = pd.read_csv(data_path/'CheXpert-v1.0-small/valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chexpert_targets = ['No Finding',\n",
    "       'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
    "       'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "       'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture',\n",
    "       'Support Devices']\n",
    "\n",
    "#chexpert_targets = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Pleural Effusion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncertainty Approaches\n",
    "\n",
    "The CheXpert paper outlines several different approaches to mapping using the uncertainty labels in the data:\n",
    "\n",
    "- Ignoring - essentially removing from the calculation in the loss function\n",
    "- Binary mapping - sending uncertain values to either 0 or 1\n",
    "- Prevalence mapping - use the rate of prevelance of the feature as it's target value\n",
    "- Self-training - consider the uncertain values as unlabeled\n",
    "- 3-Class Classification - retain a separate value for uncertain and try to predict it as a class in its own right\n",
    "\n",
    "\n",
    "The paper gives the results of different experiments with the above approaches and indicates the most accurate approach for each feature.\n",
    "    \n",
    "|Approach/Feature|Atelectasis|Cardiomegaly|Consolidation|Edema|PleuralEffusion|\n",
    "|-----------|-----------|-----------|-----------|-----------|-----------|\n",
    "|`U-Ignore`|0.818(0.759,0.877)|0.828(0.769,0.888)|0.938(0.905,0.970)|0.934(0.893,0.975)|0.928(0.894,0.962)|\n",
    "|`U-Zeros`|0.811(0.751,0.872)|0.840(0.783,0.897)|0.932(0.898,0.966)|0.929(0.888,0.970)|0.931(0.897,0.965)|\n",
    "|`U-Ones`|**0.858(0.806,0.910)**|0.832(0.773,0.890)|0.899(0.854,0.944)|0.941(0.903,0.980)|0.934(0.901,0.967)|\n",
    "|`U-Mean`|0.821(0.762,0.879)|0.832(0.771,0.892)|0.937(0.905,0.969)|0.939(0.902,0.975)|0.930(0.896,0.965)|\n",
    "|`U-SelfTrained`|0.833(0.776,0.890)|0.831(0.770,0.891)|0.939(0.908,0.971)|0.935(0.896,0.974)|0.932(0.899,0.966)|\n",
    "|`U-MultiClass`|0.821(0.763,0.879)|**0.854(0.800,0.909)**|0.937(0.905,0.969)|0.928(0.887,0.968)|0.936(0.904,0.967)|\n",
    "\n",
    "The binary mapping approaches (U-Ones and U-Zeros) are easiest to implement and so to begin with we take the best option between U-Ones and U-Zeros for each feature\n",
    "\n",
    "- Atelectasis `U-Ones`\n",
    "- Cardiomegaly `U-Zeros`\n",
    "- Consolidation `U-Zeros`\n",
    "- Edema `U-Ones`\n",
    "- Pleural Effusion `U-Zeros`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_one_features = ['Atelectasis', 'Edema']\n",
    "u_zero_features = ['Cardiomegaly', 'Consolidation', 'Pleural Effusion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_string(row):\n",
    "    feature_list = []\n",
    "    for feature in u_one_features:\n",
    "        if row[feature] in [-1,1]:\n",
    "            feature_list.append(feature)\n",
    "            \n",
    "    for feature in u_zero_features:\n",
    "        if row[feature] == 1:\n",
    "            feature_list.append(feature)\n",
    "            \n",
    "    return ';'.join(feature_list)\n",
    "            \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_df['train_valid'] = False\n",
    "full_valid_df['train_valid'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create patient and study columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_df['patient'] = full_train_df.Path.str.split('/',3,True)[2]\n",
    "full_train_df  ['study'] = full_train_df.Path.str.split('/',4,True)[3]\n",
    "\n",
    "full_valid_df['patient'] = full_valid_df.Path.str.split('/',3,True)[2]\n",
    "full_valid_df  ['study'] = full_valid_df.Path.str.split('/',4,True)[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to get the number of study and the number of patient for each entry of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.concat([full_train_df, full_valid_df])\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a feature string containing all the classes and omiting the unknowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['feature_string'] = full_df.apply(feature_string,axis = 1).fillna('')\n",
    "full_df['feature_string'] =full_df['feature_string'] .apply(lambda x:x.split(\";\"))\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the first 5 whale images\n",
    "paths =  full_df.Path[:3]\n",
    "labels = full_df.feature_string[:3]\n",
    "\n",
    "fig, m_axs = plt.subplots(1, len(labels), figsize = (20, 10))\n",
    "#show the images and label them\n",
    "for ii, c_ax in enumerate(m_axs):\n",
    "    c_ax.imshow(cv2.imread(os.path.join(data_path,paths[ii])))\n",
    "    c_ax.set_title(labels[ii])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the dataframe for training: \n",
    "* Separate the validation and the training samples\n",
    "* Take a unique sample per patient\n",
    "* Get a samle of 5% of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_perc = 0.00\n",
    "train_only_df = full_df[~full_df.train_valid]\n",
    "valid_only_df = full_df[full_df.train_valid]\n",
    "unique_patients = train_only_df.patient.unique()\n",
    "mask = np.random.rand(len(unique_patients)) <= sample_perc\n",
    "sample_patients = unique_patients[mask]\n",
    "\n",
    "dev_df = train_only_df[full_train_df.patient.isin(sample_patients)]\n",
    "train_df = train_only_df[~full_train_df.patient.isin(sample_patients)]\n",
    "\n",
    "print(valid_only_df.Path.size)\n",
    "print(train_df.Path.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen=image.ImageDataGenerator(rescale=1./255, \n",
    "                                 featurewise_center=True,\n",
    "                                 featurewise_std_normalization=True,\n",
    "                                 rotation_range=5,\n",
    "                                 width_shift_range=0.2,\n",
    "                                 height_shift_range=0.2,\n",
    "                                 horizontal_flip=True,\n",
    "                                 validation_split = 0.1)\n",
    "test_datagen=image.ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use this code to get a subset of dataset otherwise just use the whole dataset, comment this cell only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to use a subset of your dataset, uncomment if you want to use full dataset, this is for testing only\n",
    "# Limit the number of images for training\n",
    "train_df = train_df[:1000]\n",
    "# If you have a separate DataFrame for testing, limit it as well\n",
    "valid_only_df = valid_only_df[:100]\n",
    "print(valid_only_df.Path.size)\n",
    "print(train_df.Path.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remaining code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_datasets(image_size = 224):\n",
    "# change batchsize here according to requirements\n",
    "    train_generator=datagen.flow_from_dataframe(dataframe=train_df, directory=data_path, \n",
    "                                                x_col=\"Path\", y_col=\"feature_string\", has_ext=True, seed = 42, classes = chexpert_targets,\n",
    "                                                class_mode=\"categorical\", target_size=(image_size,image_size), batch_size=20, subset = \"training\")\n",
    "\n",
    "    validation_generator = datagen.flow_from_dataframe(dataframe=train_df, directory=data_path, \n",
    "                                                       x_col=\"Path\", y_col=\"feature_string\", has_ext=True, seed = 42, classes = chexpert_targets,\n",
    "                                                       class_mode=\"categorical\", target_size=(image_size,image_size), batch_size=20, subset = \"validation\")\n",
    "\n",
    "    test_generator = test_datagen.flow_from_dataframe(dataframe=valid_only_df, directory=data_path, \n",
    "                                                      target_size=(image_size,image_size),class_mode='categorical',\n",
    "                                                      batch_size=1, shuffle=False, classes = chexpert_targets,\n",
    "                                                      x_col=\"Path\", y_col=\"feature_string\")\n",
    "    \n",
    "    return [train_generator,validation_generator,test_generator]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import *\n",
    "\n",
    "class CyclicLR(Callback):\n",
    "    \n",
    "    def __init__(self, base_lr=0.001, max_lr=0.01, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc(y_true, y_pred):\n",
    "    auc = tf.metrics.auc(y_true, y_pred)[1]\n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "    return auc\n",
    "def f1_score(y_true, y_pred):\n",
    "    precision = tf.metrics.precision(y_true, y_pred)[1]\n",
    "    recall = tf.metrics.recall(y_true, y_pred)[1]\n",
    "    f1 = 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "    \n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_keras import vit\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, GlobalAveragePooling2D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "def build_model(image_size = 224):\n",
    "    vit_model = vit.vit_b16(\n",
    "        image_size = image_size,\n",
    "        activation = 'sigmoid',\n",
    "        pretrained = True,\n",
    "        include_top = False,\n",
    "        pretrained_top = False,\n",
    "    )\n",
    "    \n",
    "    x = vit_model.output\n",
    "    x = Flatten()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(128, activation=tfa.activations.gelu)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(64, activation=tfa.activations.gelu)(x)\n",
    "    x = Dense(32, activation=tfa.activations.gelu)(x)\n",
    "    predictions = Dense(14, activation='softmax')(x)  # number of classes are defined (14, in this case.)\n",
    "    \n",
    "    model = Model(inputs=vit_model.input, outputs=predictions)\n",
    "    \n",
    "    adamOptimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=10**-8, amsgrad=False)\n",
    "    model.compile(optimizer=adamOptimizer, loss='categorical_crossentropy', metrics=['accuracy','AUC'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(clr_triangular, model , datasets, epochs=1, image_size = 224):\n",
    "    \n",
    "    checkpointer = ModelCheckpoint(filepath='weights.hdf5', \n",
    "                                   verbose=1, save_best_only=True)\n",
    "    \n",
    "    train_generator,validation_generator,test_generator = datasets\n",
    "    \n",
    "    STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
    "    STEP_SIZE_VALID=validation_generator.n//validation_generator.batch_size\n",
    "    print(STEP_SIZE_TRAIN)\n",
    "    print(STEP_SIZE_VALID)\n",
    "    #return model\n",
    "    return model.fit_generator(generator=train_generator,\n",
    "                        steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                        validation_data=validation_generator,\n",
    "                        validation_steps=STEP_SIZE_VALID,\n",
    "                        epochs=epochs, callbacks = [clr_triangular, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size_input = 224\n",
    "model = build_model(image_size = image_size_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = generate_datasets(image_size = image_size_input)\n",
    "train_generator,validation_generator,test_generator = datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change number of epochs here if you want to train for more epochs\n",
    "clr_triangular = CyclicLR(mode='exp_range')\n",
    "history = train_model(clr_triangular, model , datasets, epochs=1, image_size = image_size_input) # epochs=50 or more for better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clr_triangular.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "test = pd.Series(test_generator.labels)\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_labels = mlb.fit_transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator.reset()\n",
    "y_pred_keras = model.predict_generator(test_generator,verbose = 1,steps=test_generator.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "plt.figure(1,figsize=(10,10))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "#for ii in range(1,y_pred_keras.shape[1]):\n",
    "for ii in range(1,5):\n",
    "    fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_labels[:,ii], y_pred_keras[:,ii])\n",
    "    auc_keras = auc(fpr_keras, tpr_keras)\n",
    "    plt.plot(fpr_keras, tpr_keras, label=chexpert_targets[ii-1] + '(area = {:.3f})'.format(auc_keras))\n",
    "    \n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
